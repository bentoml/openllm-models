alias:
- large
build:
  post:
  - uv pip install --compile-bytecode torch
  - curl -L -o ./causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
    https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.5.0.post8/causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
  - uv pip install --compile-bytecode ./causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
  - curl -L -o ./mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
    https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
  - uv pip install --compile-bytecode ./mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
  system_packages:
  - curl
  - git
engine_config:
  enable_prefix_caching: false
  max_model_len: 225280
  model: ai21labs/AI21-Jamba-1.5-Large
  quantization: experts_int8
  tensor_parallel_size: 8
project: vllm-chat
server_config:
  tool_call_parser: jamba
service_config:
  envs:
  - name: HF_TOKEN
  - name: UV_NO_BUILD_ISOLATION
    value: 1
  name: jamba1.5
  resources:
    gpu: 8
    gpu_type: nvidia-a100-80gb
  traffic:
    timeout: 300
