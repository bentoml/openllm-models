envs:
- name: CMAKE_ARGS
  value: -DLLAMA_METAL=on
include:
- '*.py'
- '*.yaml'
- ui/*
labels:
  model_name: microsoft/Phi-3-mini-4k-instruct-gguf
  openllm_alias: 3.8b-ggml-q4,3.8b-mini-instruct-4k-ggml-q4
  platforms: macos
  source: https://github.com/bentoml/openllm-models-feed/tree/main/src/llamacpp-chat
python:
  lock_packages: true
  requirements_txt: ./requirements.txt
service: service:LlamaCppChat
