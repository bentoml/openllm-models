conda:
  channels: null
  dependencies: null
  environment_yml: null
  pip: null
description: null
docker:
  base_image: null
  cuda_version: null
  distro: debian
  dockerfile_template: null
  env:
    CMAKE_ARGS: -DLLAMA_METAL=on
  python_version: '3.9'
  setup_script: null
  system_packages: null
envs:
- name: CMAKE_ARGS
  value: -DLLAMA_METAL=on
exclude: []
include:
- '*.py'
- ui/*
labels:
  model_name: microsoft/Phi-3-mini-4k-instruct-gguf
  openllm_alias: 3.8b-ggml-q4,3.8b-mini-instruct-4k-ggml-q4
  platforms: macos,linux
  source: https://github.com/bentoml/openllm-models-feed/tree/main/src/llamacpp-chat
models: []
name: null
python:
  extra_index_url: null
  find_links: null
  index_url: null
  lock_packages: true
  no_index: null
  pack_git_packages: true
  packages: null
  pip_args: null
  requirements_txt: ./requirements.txt
  trusted_host: null
  wheels: null
service: service:LlamaCppChat
