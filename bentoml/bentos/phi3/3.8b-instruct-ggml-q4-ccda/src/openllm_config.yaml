engine_config:
  filename: Phi-3-mini-4k-instruct-q4.gguf
  max_model_len: 2048
  repo_id: microsoft/Phi-3-mini-4k-instruct-gguf
extra_envs:
- name: CMAKE_ARGS
  value: -DGGML_METAL=on
extra_labels:
  model_name: microsoft/Phi-3-mini-4k-instruct-gguf
  openllm_alias: 3.8b-ggml-q4,3.8b-mini-instruct-4k-ggml-q4
  platforms: macos
project: llamacpp-chat
service_config:
  name: phi3
  resources:
    memory: 3Gi
  traffic:
    timeout: 300
