conda:
  channels: null
  dependencies: null
  environment_yml: null
  pip: null
description: null
docker:
  base_image: null
  cuda_version: null
  distro: debian
  dockerfile_template: null
  env:
    HF_TOKEN: ''
    VLLM_ATTENTION_BACKEND: FLASHINFER
  python_version: '3.9'
  setup_script: null
  system_packages: null
envs:
- name: HF_TOKEN
- name: VLLM_ATTENTION_BACKEND
  value: FLASHINFER
exclude: []
include:
- '*.py'
- ui/*
- ui/chunks/*
- ui/css/*
- ui/media/*
- ui/chunks/pages/*
- bentovllm_openai/*.py
- chat_templates/chat_templates/*.jinja
- chat_templates/generation_configs/*.json
labels:
  model_name: google/gemma-2-9b-it
  openllm_alias: 9b,9b-instruct
  platforms: linux
  source: https://github.com/bentoml/openllm-models-feed/tree/main/src/vllm-chat
models: []
name: null
python:
  extra_index_url: null
  find_links: null
  index_url: null
  lock_packages: true
  no_index: null
  pack_git_packages: true
  packages: null
  pip_args: null
  requirements_txt: ./requirements.txt
  trusted_host: null
  wheels: null
service: service:VLLM
