alias:
- 2b
engine_config:
  dtype: half
  enable_prefix_caching: false
  max_model_len: 2048
  model: google/gemma-2-2b-it
project: vllm-chat
server_config:
  chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages\
    \ = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip()\
    \ + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set\
    \ system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n\
    \    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{\
    \ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...')\
    \ }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content\
    \ = system_message + message['content'] %}\n    {% else %}\n        {% set content\
    \ = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant')\
    \ %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role =\
    \ message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\\
    n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role']\
    \ == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n\
    \    {% endif %}\n{% endfor %}\n"
service_config:
  envs:
  - name: HF_TOKEN
  name: gemma2
  resources:
    gpu: 1
    gpu_type: nvidia-rtx-3060
  traffic:
    timeout: 300
