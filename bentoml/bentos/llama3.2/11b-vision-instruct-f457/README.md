# OpenLLM service for meta-llama/Llama-3.2-11B-Vision-Instruct

- ðŸ“¸ Image-to-text capabilities for multimodal inputs
- âš¡ High-performance inference using vLLM
- ðŸ’¬ Streaming responses for chat completions
- ðŸ”„ OpenAI-compatible API for easy integration
- ðŸŽ¨ Chat UI for interacting with the model

## API Endpoints

- `/v1/chat/completions` - For chat completions
- `/v1/models` - To list available models

## Service Configuration

This model was configured with the following settings:

```yaml
{
  "enforce_eager": true,
  "limit_mm_per_prompt": {
    "image": 1
  },
  "max_model_len": 16384,
  "max_num_seqs": 16,
  "tensor_parallel_size": 1,
  "tool_call_parser": "pythonic"
}
```

## Usage

To use this model, you can interact with it using the OpenAI API format. Here's a sample:

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:3000/v1",
    api_key="dummy"
)

for chunk in client.chat.completions.create(
    model="meta-llama/Llama-3.2-11B-Vision-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me a joke about programming."}
    ],
    stream=True
):
  if chunk.choices[0].delta.content: print(chunk.choices[0].delta.content, end="")
```

## Deployment

```bash
openllm deploy llama3.2:11b
```