engine_config:
  additional_files:
  - qwen2.5-72b-instruct-q4_k_m-00001-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00002-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00003-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00004-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00005-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00006-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00007-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00008-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00009-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00010-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00011-of-00012.gguf
  - qwen2.5-72b-instruct-q4_k_m-00012-of-00012.gguf
  filename: qwen2.5-72b-instruct-q4_k_m-00001-of-00012.gguf
  max_model_len: 2048
  repo_id: Qwen/Qwen2.5-72B-Instruct-GGUF
labels:
  platforms: darwin
project: llamacpp-chat
service_config:
  envs:
  - name: CMAKE_ARGS
    value: -DGGML_METAL=on
  name: qwen2.5
  resources:
    memory: 60Gi
  traffic:
    timeout: 300
