conda:
  channels: null
  dependencies: null
  environment_yml: null
  pip: null
description: null
docker:
  base_image: null
  cuda_version: null
  distro: debian
  dockerfile_template: null
  env:
    CMAKE_ARGS: -DGGML_METAL=on
  python_version: '3.9'
  setup_script: null
  system_packages: null
envs:
- name: CMAKE_ARGS
  value: -DGGML_METAL=on
exclude: []
include:
- LICENCE
- '*.py'
- '*.yaml'
- '*.txt'
- ui/*
labels:
  model_name: Qwen/Qwen2.5-14B-Instruct-GGUF
  openllm_alias: 14b-ggml-q8
  platforms: macos
models: []
name: qwen2.5
python:
  extra_index_url: null
  find_links: null
  index_url: null
  lock_packages: true
  no_index: null
  pack_git_packages: true
  packages:
  - bentoml>=1.3.20
  - kantoku>=0.18.1
  - huggingface-hub
  - llama_cpp_python==0.3.1
  - fastapi
  pip_args: null
  requirements_txt: null
  trusted_host: null
  wheels: null
service: service:LlamaCppChat
