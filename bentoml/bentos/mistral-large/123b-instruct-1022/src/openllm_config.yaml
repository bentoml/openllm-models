engine_config:
  dtype: half
  max_model_len: 2048
  model: mistralai/Mistral-Large-Instruct-2407
  tensor_parallel_size: 4
extra_envs:
- name: HF_TOKEN
project: vllm-chat
server_config:
  chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages\
    \ = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip()\
    \ + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set\
    \ system_message = '' %}\n{% endif %}\n\n{{ bos_token }}\n{% for message in loop_messages\
    \ %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n   \
    \     {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...')\
    \ }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content\
    \ = system_message + message['content'] %}\n    {% else %}\n        {% set content\
    \ = message['content'] %}\n    {% endif %}\n\n    {% if message['role'] == 'user'\
    \ %}\n        {{ '[INST] ' + content.strip() + ' [/INST]' }}\n    {% elif message['role']\
    \ == 'assistant' %}\n        {{ ' ' + content.strip() + eos_token }}\n    {% endif\
    \ %}\n{% endfor %}\n"
service_config:
  name: mistral-large
  resources:
    gpu: 4
    gpu_type: nvidia-a100-80gb
  traffic:
    timeout: 300
