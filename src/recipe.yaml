'phi3:3.8b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: phi3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: microsoft/Phi-3-mini-4k-instruct
    max_model_len: 4096
    dtype: half
  chat_template: phi-3
  extra_labels:
    openllm_alias: 3.8b,3.8b-mini,3.8b-mini-instruct-4k-fp16
    model_name: microsoft/Phi-3-mini-4k-instruct
'llama2:7b-chat-fp16':
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-t4
  extra_envs:
    - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-2-7b-chat-hf
    max_model_len: 1024
    dtype: half
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 7b,7b-chat
    model_name: meta-llama/Llama-2-7b-chat-hf
'llama2:13b-chat-fp16':
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
  extra_envs:
    - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-2-13b-chat-hf
    max_model_len: 1024
    dtype: half
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 13b,13b-chat
    model_name: meta-llama/Llama-2-13b-chat-hf
'llama2:70b-chat-fp16':
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  extra_envs:
    - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-2-70b-chat-hf
    max_model_len: 1024
    dtype: half
    tensor_parallel_size: 2
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 70b,70b-chat
    model_name: meta-llama/Llama-2-70b-chat-hf
'llama2:7b-chat-awq-4bit':
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: TheBloke/Llama-2-7B-Chat-AWQ
    max_model_len: 1024
    quantization: awq
    enforce_eager: true
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 7b-4bit,7b-chat-4bit
    model_name: TheBloke/Llama-2-7B-Chat-AWQ
'mistral:7b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: TheBloke/Mistral-7B-Instruct-v0.1-AWQ
    max_model_len: 1024
    quantization: awq
    enforce_eager: true
    dtype: half
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 7b-4bit,7b-instruct-4bit
    model_name: TheBloke/Mistral-7B-Instruct-v0.1-AWQ
'mistral:7b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: mistralai/Mistral-7B-Instruct-v0.1
    max_model_len: 1024
    enforce_eager: true
    dtype: half
  extra_envs:
    - name: HF_TOKEN
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 7b,7b-instruct
    model_name: mistralai/Mistral-7B-Instruct-v0.1
'mistral:24b-instruct-nemo':
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: mistralai/Mistral-Nemo-Instruct-2407
    max_model_len: 2048
    enforce_eager: true
    dtype: half
  chat_template: mistral-instruct
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: nemo,nemo-instruct
    model_name: mistralai/Mistral-Nemo-Instruct-2407
'codestral:22b-v0.1-fp16':
  project: vllm-chat
  service_config:
    name: codestral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: mistralai/Codestral-22B-v0.1
    max_model_len: 8192
    enforce_eager: true
    dtype: half
  chat_template: mistral-instruct
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 22b,22b-v0.1
    model_name: mistralai/Codestral-22B-v0.1
'mistral-large:123b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: mistral-large
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80g
  engine_config:
    model: mistralai/Mistral-Large-Instruct-2407
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 4
  chat_template: mistral-instruct
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 123b, 123b-instruct-2407
    model_name: mistralai/Mistral-Large-Instruct-2407
'mistral-large:123b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: mistral-large
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: casperhansen/mistral-large-instruct-2407-awq
    max_model_len: 2048
    dtype: half
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 123b-4bit,123b-instruct-2407-4bit
    model_name: casperhansen/mistral-large-instruct-2407-awq
'llama3:8b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: casperhansen/llama-3-8b-instruct-awq
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 8b-4bit,8b-instruct-4bit
    model_name: casperhansen/llama-3-8b-instruct-awq
'llama3:70b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: casperhansen/llama-3-70b-instruct-awq
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 70b-4bit,70b-instruct-4bit
    model_name: casperhansen/llama-3-70b-instruct-awq
'llama3:8b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: meta-llama/Meta-Llama-3-8B-Instruct
    max_model_len: 2048
    dtype: half
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 8b,8b-instruct
    model_name: meta-llama/Meta-Llama-3-8B-Instruct
'llama3:70b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: meta-llama/Meta-Llama-3-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 70b,70b-instruct
    model_name: meta-llama/Meta-Llama-3-70B-Instruct
'llama3.2:1b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: meta-llama/Llama-3.2-1B-Instruct
    max_model_len: 2048
    dtype: half
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 1b,1b-instruct
    model_name: meta-llama/Llama-3.2-1B-Instruct
'llama3.2:3b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: meta-llama/Llama-3.2-3B-Instruct
    max_model_len: 2048
    dtype: half
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 3b,3b-instruct
    model_name: meta-llama/Llama-3.2-3B-Instruct
'llama3.1:8b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  extra_envs:
    - name: HF_TOKEN
  engine_config:
    model: meta-llama/Meta-Llama-3.1-8B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 8b,8b-instruct
    model_name: meta-llama/Meta-Llama-3.1-8B-Instruct
'llama3.1:8b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 8b-4bit,8b-instruct-4bit
    model_name: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
'llama3.1:70b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: meta-llama/Meta-Llama-3.1-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 70b,70b-instruct
    model_name: meta-llama/Meta-Llama-3.1-70B-Instruct
'llama3.1:70b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 70b-4bit,70b-instruct-4bit
    model_name: hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
'llama3.1:405b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80g
  engine_config:
    model: hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4
    max_model_len: 2048
    quantization: awq
    tensor_parallel_size: 4
  extra_labels:
    openllm_alias: 405b-4bit,405b-instruct-4bit
    model_name: hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4
'gemma2:9b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: google/gemma-2-9b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 9b,9b-instruct
    model_name: google/gemma-2-9b-it
  extra_envs:
    - name: VLLM_ATTENTION_BACKEND
      value: FLASHINFER
    - name: HF_TOKEN
  extra_requirements:
    - --extra-index-url https://flashinfer.ai/whl/cu121/torch2.3
    - flashinfer==0.1.2+cu121torch2.3
'gemma2:27b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: google/gemma-2-27b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 27b,27b-instruct
    model_name: google/gemma-2-27b-it
  extra_envs:
    - name: VLLM_ATTENTION_BACKEND
      value: FLASHINFER
    - name: HF_TOKEN
  extra_requirements:
    - --extra-index-url https://flashinfer.ai/whl/cu121/torch2.3
    - flashinfer==0.1.2+cu121torch2.3
'gemma:2b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: gemma
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: google/gemma-2b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 2b,2b-instruct
    model_name: google/gemma-2b-it
  extra_envs:
    - name: HF_TOKEN
'gemma:7b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: gemma
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: google/gemma-7b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 7b,7b-instruct
    model_name: google/gemma-7b-it
  extra_envs:
    - name: HF_TOKEN
'gemma:7b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: gemma
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: casperhansen/gemma-7b-it-awq
    max_model_len: 2048
    quantization: awq
  chat_template: gemma-it
  extra_labels:
    openllm_alias: 7b-4bit,7b-instruct-4bit
    model_name: casperhansen/gemma-7b-it-awq
'mixtral:8x7b-instruct-v0.1-fp16':
  project: vllm-chat
  service_config:
    name: mixtral
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    max_model_len: 2048
    tensor_parallel_size: 2
  chat_template: mistral-instruct
  extra_envs:
    - name: HF_TOKEN
  extra_labels:
    openllm_alias: 8x7b,8x7b-instruct
    model_name: mistralai/Mixtral-8x7B-Instruct-v0.1
'mixtral:8x7b-instruct-v0.1-awq-4bit':
  project: vllm-chat
  service_config:
    name: mixtral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
  engine_config:
    model: casperhansen/mixtral-instruct-awq
    max_model_len: 2048
    quantization: awq
    gpu_memory_utilization: 0.8
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 8x7b-4bit
    model_name: casperhansen/mixtral-instruct-awq
'qwen2.5:0.5b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2.5-0.5B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 0.5b,0.5b-instruct
    model_name: Qwen/Qwen2.5-0.5B-Instruct
'qwen2.5:1.5b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2.5-1.5B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 1.5b,1.5b-instruct
    model_name: Qwen/Qwen2.5-1.5B-Instruct
'qwen2.5:3b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2.5-3B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 3b,3b-instruct
    model_name: Qwen/Qwen2.5-3B-Instruct
'qwen2.5:7b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-7B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 7b,7b-instruct
    model_name: Qwen/Qwen2.5-7B-Instruct
'qwen2.5:14b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2.5-14B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 14b,14b-instruct
    model_name: Qwen/Qwen2.5-14B-Instruct
'qwen2.5:32b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2.5-32B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 32b,32b-instruct
    model_name: Qwen/Qwen2.5-32B-Instruct
'qwen2.5:72b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 72b,72b-instruct
    model_name: Qwen/Qwen2.5-72B-Instruct
'qwen2:0.5b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2-0.5B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 0.5b,0.5b-instruct
    model_name: Qwen/Qwen2-0.5B-Instruct
'qwen2:1.5b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2-1.5B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 1.5b,1.5b-instruct
    model_name: Qwen/Qwen2-1.5B-Instruct
'qwen2:7b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2-7B-Instruct-AWQ
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 7b-4bit,7b-instruct-4bit
    model_name: Qwen/Qwen2-7B-Instruct-AWQ
'qwen2:7b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: Qwen/Qwen2-7B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 7b,7b-instruct
    model_name: Qwen/Qwen2-7B-Instruct
'qwen2:72b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2-72B-Instruct-AWQ
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 72b-4bit,72b-instruct-4bit
    model_name: Qwen/Qwen2-72B-Instruct-AWQ
'qwen2:57b-a14b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2-57B-A14B-Instruct
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 57b-a14b,57b-a14b-instruct
    model_name: Qwen/Qwen2-57B-A14B-Instruct
'qwen2:72b-instruct-fp16':
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2-72B-Instruct
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 72b,72b-instruct
    model_name: Qwen/Qwen2-72B-Instruct
'phi3:3.8b-instruct-ggml-q4':
  project: llamacpp-chat
  service_config:
    name: phi3
    traffic:
      timeout: 300
    resources:
      memory: 3Gi
  engine_config:
    model: microsoft/Phi-3-mini-4k-instruct-gguf
    max_model_len: 2048
    filename: "Phi-3-mini-4k-instruct-q4.gguf"
  extra_labels:
    openllm_alias: 3.8b-ggml-q4,3.8b-mini-instruct-4k-ggml-q4
    model_name: microsoft/Phi-3-mini-4k-instruct-gguf
    platforms: macos
'llama3.2:11b-vision-instruct':
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: meta-llama/Llama-3.2-11B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
  extra_labels:
    openllm_alias: 11b-vision
    model_name: meta-llama/Llama-3.2-11B-Vision-Instruct
  extra_envs:
    - name: HF_TOKEN
'pixtral:12b-240910':
  project: vllm-chat
  service_config:
    name: pixtral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: mistral-community/pixtral-12b-240910
    tokenizer_mode: mistral
    enable_prefix_caching: true
    enable_chunked_prefill: false
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
  extra_labels:
    openllm_alias: 12b, 12b-vision
    model_name: mistral-community/pixtral-12b-240910
  extra_requirements:
    - mistral_common[opencv]
'llama3.2:1b-instruct-fp16-ggml-darwin':
  project: llamacpp-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      cpu: 1
      memory: 3Gi
  engine_config:
    model: unsloth/Llama-3.2-1B-Instruct-GGUF
    max_model_len: 2048
    filename: "Llama-3.2-1B-Instruct-F16.gguf"
  extra_labels:
    openllm_alias: llama3.2,1b-instruct-fp16-ggml-darwin
    model_name: unsloth/Llama-3.2-1B-Instruct-GGUF
    platforms: macos
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_METAL=on"
'llama3.2:1b-instruct-fp16-ggml-linux':
  project: llamacpp-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      cpu: 1
      memory: 3Gi
  engine_config:
    model: unsloth/Llama-3.2-1B-Instruct-GGUF
    max_model_len: 2048
    filename: "Llama-3.2-1B-Instruct-F16.gguf"
  extra_labels:
    openllm_alias: llama3.2,1b-instruct-fp16-ggml-linux
    model_name: unsloth/Llama-3.2-1B-Instruct-GGUF
    platforms: linux
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"