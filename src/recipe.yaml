"phi4:14b":
  project: vllm-chat
  service_config:
    name: phi4
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: microsoft/phi-4
    max_model_len: 8192
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set offset = 1 %}\n{% else %}\n    {% set offset = 0 %}\n{% endif %}\n\n{% for message in messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == offset) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {{ '<|' + message['role'] + '|>\\n' + message['content'].strip() + '<|end|>' + '\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{ '<|assistant|>\\n' }}\n    {% endif %}\n{% endfor %}\n"
"pixtral:12b-2409":
  project: vllm-chat
  vision: true
  service_config:
    name: pixtral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Pixtral-12B-2409
    tokenizer_mode: mistral
    enable_prefix_caching: false
    enable_chunked_prefill: false
    limit_mm_per_prompt:
      image: 5
    max_model_len: 32768
  requirements:
    - mistral_common[opencv]
"pixtral:124b-2411":
  project: vllm-chat
  vision: true
  service_config:
    name: pixtral
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Pixtral-Large-Instruct-2411
    tokenizer_mode: mistral
    enable_chunked_prefill: false
    enable_prefix_caching: false
    limit_mm_per_prompt:
      image: 5
    max_model_len: 16384
    dtype: half
    tensor_parallel_size: 4
  requirements:
    - mistral_common[opencv]
"mistral:8b-instruct":
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Ministral-8B-Instruct-2410
    dtype: half
  build:
    exclude:
      - "consolidated*"
"mistral:24b-small-instruct-2501":
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Mistral-Small-24B-Instruct-2501
  build:
    exclude:
      - "consolidated*"
"mistral-large:123b-instruct-2407":
  project: vllm-chat
  build:
    exclude:
      - "consolidated*"
  service_config:
    name: mistral-large
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Mistral-Large-Instruct-2407
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 4
"gemma2:2b-instruct":
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
    envs:
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-2b-it
    max_model_len: 2048
    dtype: half
    enable_prefix_caching: false
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
"gemma2:9b-instruct":
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-9b-it
    max_model_len: 2048
    dtype: half
    enable_prefix_caching: false
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
"gemma2:27b-instruct":
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-27b-it
    max_model_len: 2048
    dtype: half
    enable_prefix_caching: false
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
"qwen2.5:7b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-7B-Instruct
    max_model_len: 2048
"qwen2.5:14b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-14B-Instruct
    max_model_len: 2048
"qwen2.5:14b-instruct-awq":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-14B-Instruct-AWQ
    max_model_len: 4096
"qwen2.5:14b-instruct-gptq-w4a16":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4
    max_model_len: 4096
"qwen2.5:14b-instruct-gptq-w8a8":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4
    max_model_len: 4096
"qwen2.5:32b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-32B-Instruct
    max_model_len: 2048
"qwen2.5:32b-instruct-awq":
  project: vllm-chat
  engine_config:
    max_model_len: 20480
    model: Qwen/Qwen2.5-32B-Instruct-AWQ
  service_config:
    name: qwen2.5
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
  server_config:
    tool_call_parser: "llama3_json"
"qwen2.5:32b-instruct-gptq-w4a16":
  project: vllm-chat
  engine_config:
    max_model_len: 20480
    model: Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
  service_config:
    name: qwen2.5
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
  server_config:
    tool_call_parser: "llama3_json"
"qwen2.5:32b-instruct-gptq-w8a8":
  project: vllm-chat
  engine_config:
    max_model_len: 8192
    model: Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8
  service_config:
    name: qwen2.5
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
  server_config:
    tool_call_parser: "llama3_json"
"qwen2.5:72b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct
    tensor_parallel_size: 2
    max_model_len: 2048
"qwen2.5:72b-instruct-awq":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct-AWQ
    max_model_len: 8192
"qwen2.5:72b-instruct-gptq-w4a16":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8
    max_model_len: 4096
"qwen2.5:72b-instruct-gptq-w8a8":
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8
    max_model_len: 8192
"qwen2.5-coder:3b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-3B-Instruct
    max_model_len: 8192
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:7b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-7B-Instruct
    max_model_len: 8192
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:7b-instruct-awq":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
    max_model_len: 16384
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:7b-instruct-gptq-w4a16":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4
    max_model_len: 16384
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:7b-instruct-gptq-w8a16":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8
    max_model_len: 8192
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:14b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-14B-Instruct
    max_model_len: 4096
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:14b-instruct-awq":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-14B-Instruct-AWQ
    max_model_len: 16384
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:14b-instruct-gptq-w4a16":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4
    max_model_len: 16384
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:14b-instruct-gptq-w8a8":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8
    max_model_len: 8192
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5-coder:32b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-32B-Instruct
    max_model_len: 8192
  server_config:
    enable_auto_tool_choice: true
    enable_tool_call_parser: true
    tool_call_parser: "llama3_json"
"qwen2.5:72b-instruct-ggml-q4-darwin":
  project: llamacpp-chat
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-72b-instruct-q4_k_m-00001-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00002-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00003-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00004-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00005-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00006-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00007-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00008-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00009-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00010-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00011-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00012-of-00012.gguf
    filename: qwen2.5-72b-instruct-q4_k_m-00001-of-00012.gguf
    repo_id: Qwen/Qwen2.5-72B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
    envs:
      - name: CMAKE_ARGS
        value: "-DGGML_METAL=on"
  labels:
    platforms: darwin
"qwen2.5:32b-instruct-ggml-darwin":
  project: llamacpp-chat
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-32b-instruct-00001-of-00017.gguf
      - qwen2.5-32b-instruct-00002-of-00017.gguf
      - qwen2.5-32b-instruct-00003-of-00017.gguf
      - qwen2.5-32b-instruct-00004-of-00017.gguf
      - qwen2.5-32b-instruct-00005-of-00017.gguf
      - qwen2.5-32b-instruct-00006-of-00017.gguf
      - qwen2.5-32b-instruct-00007-of-00017.gguf
      - qwen2.5-32b-instruct-00008-of-00017.gguf
      - qwen2.5-32b-instruct-00009-of-00017.gguf
      - qwen2.5-32b-instruct-00010-of-00017.gguf
      - qwen2.5-32b-instruct-00011-of-00017.gguf
      - qwen2.5-32b-instruct-00012-of-00017.gguf
      - qwen2.5-32b-instruct-00013-of-00017.gguf
      - qwen2.5-32b-instruct-00014-of-00017.gguf
      - qwen2.5-32b-instruct-00015-of-00017.gguf
      - qwen2.5-32b-instruct-00016-of-00017.gguf
      - qwen2.5-32b-instruct-00017-of-00017.gguf
    filename: qwen2.5-32b-instruct-00001-of-00017.gguf
    repo_id: Qwen/Qwen2.5-32B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
    envs:
      - name: CMAKE_ARGS
        value: "-DGGML_METAL=on"
  labels:
    platforms: darwin
"qwen2.5:14b-instruct-ggml-q4-darwin":
  project: llamacpp-chat
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-14b-instruct-q4_0-00001-of-00003.gguf
      - qwen2.5-14b-instruct-q4_0-00002-of-00003.gguf
      - qwen2.5-14b-instruct-q4_0-00003-of-00003.gguf
    filename: qwen2.5-14b-instruct-q4_0-00001-of-00003.gguf
    repo_id: Qwen/Qwen2.5-14B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
    envs:
      - name: CMAKE_ARGS
        value: "-DGGML_METAL=on"
  labels:
    platforms: darwin
"qwen2.5:14b-instruct-ggml-q8-darwin":
  project: llamacpp-chat
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-14b-instruct-q8_0-00001-of-00004.gguf
      - qwen2.5-14b-instruct-q8_0-00002-of-00004.gguf
      - qwen2.5-14b-instruct-q8_0-00003-of-00004.gguf
      - qwen2.5-14b-instruct-q8_0-00004-of-00004.gguf
    filename: qwen2.5-14b-instruct-q8_0-00001-of-00004.gguf
    repo_id: Qwen/Qwen2.5-14B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
    envs:
      - name: CMAKE_ARGS
        value: "-DGGML_METAL=on"
  labels:
    platforms: darwin
"qwen2.5-coder:7b-instruct-ggml-darwin":
  project: llamacpp-chat
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-coder-7b-instruct-00001-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00002-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00003-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00004-of-00004.gguf
    filename: qwen2.5-coder-7b-instruct-00001-of-00004.gguf
    repo_id: Qwen/Qwen2.5-Coder-7B-Instruct-GGUF
  service_config:
    name: qwen2.5-coder
    resources:
      memory: 16Gi
    traffic:
      timeout: 300
    envs:
      - name: CMAKE_ARGS
        value: "-DGGML_METAL=on"
  labels:
    platforms: darwin
"qwen2.5-coder:7b-instruct-ggml-linux":
  project: llamacpp-chat
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-coder-7b-instruct-00001-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00002-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00003-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00004-of-00004.gguf
    filename: qwen2.5-coder-7b-instruct-00001-of-00004.gguf
    repo_id: Qwen/Qwen2.5-Coder-7B-Instruct-GGUF
  service_config:
    name: qwen2.5-coder
    resources:
      memory: 16Gi
    traffic:
      timeout: 300
    envs:
      - name: CMAKE_ARGS
        value: "-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"
"qwq:32b":
  project: vllm-chat
  service_config:
    name: qwq
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
      - name: UV_NO_BUILD_ISOLATION
        value: 1
  engine_config:
    model: Qwen/QwQ-32B
    max_model_len: 8192
  server_config:
    enable_reasoning: true
    reasoning_parser: "deepseek_r1"
    enable_auto_tool_choice: true
    tool_call_parser: "llama3_json"
"llama3.1:8b-instruct":
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Meta-Llama-3.1-8B-Instruct
    max_model_len: 2048
  server_config:
    tool_call_parser: "llama3_json"
"llama3.2:1b-instruct":
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-1B-Instruct
    max_model_len: 8192
  server_config:
    tool_call_parser: "pythonic"
"llama3.2:3b-instruct":
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-3B-Instruct
    max_model_len: 8192
  server_config:
    tool_call_parser: "pythonic"
"llama3.2:11b-vision-instruct":
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  build:
    exclude:
      - "original"
  engine_config:
    model: meta-llama/Llama-3.2-11B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
  vision: true
  server_config:
    tool_call_parser: "pythonic"
"llama3.2:90b-vision-instruct":
  project: vllm-chat
  vision: true
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-90B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
  server_config:
    tool_call_parser: "pythonic"
  build:
    exclude:
      - "original"
"llama3.3:70b-instruct":
  engine_config:
    model: meta-llama/Llama-3.3-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
  build:
    exclude:
      - "original"
      - "consolidated*"
  server_config:
    tool_call_parser: "pythonic"
  project: vllm-chat
  service_config:
    name: llama3.3
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
"hermes-3:llama3.1-405b-instruct":
  project: vllm-chat
  engine_config:
    model: NousResearch/Hermes-3-Llama-3.1-405B-FP8
    max_model_len: 2048
    tensor_parallel_size: 6
  server_config:
    tool_call_parser: "pythonic"
  service_config:
    name: hermes3
    traffic:
      timeout: 300
    resources:
      gpu: 6
      gpu_type: nvidia-a100-80gb
"hermes-3:deep-llama3-8b":
  project: vllm-chat
  reasoning: true
  service_config:
    name: hermes
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: NousResearch/DeepHermes-3-Llama-3-8B-Preview
    max_model_len: 8192
  server_config:
    tool_call_parser: "pythonic"
"jamba1.5:mini":
  project: vllm-chat
  reasoning: true
  service_config:
    name: jamba1.5
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
      - name: UV_NO_BUILD_ISOLATION
        value: 1
  engine_config:
    model: ai21labs/AI21-Jamba-1.5-Mini
    max_model_len: 204800
    tensor_parallel_size: 2
    enable_prefix_caching: false
  server_config:
    tool_call_parser: "jamba"
  build:
    post:
      - uv pip install --compile-bytecode torch
      - uv pip install --compile-bytecode mamba-ssm[causal-conv1d]
"jamba1.5:large":
  project: vllm-chat
  service_config:
    name: jamba1.5
    traffic:
      timeout: 300
    resources:
      gpu: 8
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
      - name: UV_NO_BUILD_ISOLATION
        value: 1
  engine_config:
    model: ai21labs/AI21-Jamba-1.5-Large
    max_model_len: 225280
    tensor_parallel_size: 8
    quantization: "experts_int8"
    enable_prefix_caching: false
  build:
    post:
      - uv pip install --compile-bytecode --no-build-isolation torch
      - uv pip install --compile-bytecode --no-build-isolation torch mamba-ssm[causal-conv1d]
  server_config:
    tool_call_parser: "jamba"
"deepseek:v3-671b":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 16
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-V3
    max_model_len: 2048
    tensor_parallel_size: 16
"deepseek:r1-671b":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 16
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1
    tensor_parallel_size: 16
    trust_remote_code: True
    max_model_len: 4096
  server_config:
    enable_reasoning: true
    reasoning_parser: deepseek_r1
"deepseek:r1-distill-llama3.3-70b":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    tensor_parallel_size: 2
    max_model_len: 4096
  server_config:
    reasoning_parser: "deepseek_r1"
    tool_call_parser: "llama3_json"
"deepseek:r1-distill-llama3.3-70b-w8a8":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: neuralmagic/DeepSeek-R1-Distill-Llama-70B-quantized.w8a8
    max_model_len: 4096
  server_config:
    reasoning_parser: "deepseek_r1"
    tool_call_parser: "llama3_json"
"deepseek:r1-distill-llama3.3-70b-w4a16":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: neuralmagic/DeepSeek-R1-Distill-Llama-70B-quantized.w4a16
    max_model_len: 8192
  server_config:
    reasoning_parser: "deepseek_r1"
    tool_call_parser: "llama3_json"
"deepseek:r1-distill-qwen2.5-32b":
  project: vllm-chat
  service_config:
    name: deepseek
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    traffic:
      timeout: 300
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    max_model_len: 4096
  server_config:
    reasoning_parser: "deepseek_r1"
    tool_call_parser: "llama3_json"
"deepseek:r1-distill-qwen2.5-32b-w8a8":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: neuralmagic/DeepSeek-R1-Distill-Qwen-32B-quantized.w8a8
    max_model_len: 4096
  server_config:
    tool_call_parser: "llama3_json"
    reasoning_parser: "deepseek_r1"
"deepseek:r1-distill-qwen2.5-32b-w4a16":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: neuralmagic/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16
    max_model_len: 16384
  server_config:
    tool_call_parser: "llama3_json"
    reasoning_parser: "deepseek_r1"
"deepseek:r1-distill-qwen2.5-14b":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
    max_model_len: 4096
  server_config:
    tool_call_parser: "llama3_json"
    reasoning_parser: deepseek_r1
"deepseek:r1-distill-qwen2.5-14b-w8a8":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: neuralmagic/DeepSeek-R1-Distill-Qwen-14B-quantized.w8a8
    max_model_len: 4096
  server_config:
    tool_call_parser: "llama3_json"
    reasoning_parser: "deepseek_r1"
"deepseek:r1-distill-qwen2.5-14b-w4a16":
  project: vllm-chat
  service_config:
    name: bentovllm-r1-qwen2.5-14b-w4a16-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: neuralmagic/DeepSeek-R1-Distill-Qwen-14B-quantized.w4a16
    max_model_len: 4096
  server_config:
    tool_call_parser: "llama3_json"
    reasoning_parser: "deepseek_r1"
"deepseek:r1-distill-llama3.1-8b":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
    max_model_len: 4096
  server_config:
    tool_call_parser: "llama3_json"
    reasoning_parser: "deepseek_r1"
"deepseek:r1-distill-qwen2.5-7b-math":
  project: vllm-chat
  service_config:
    name: deepseek
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    max_model_len: 4096
  server_config:
    tool_call_parser: "llama3_json"
    reasoning_parser: "deepseek_r1"
