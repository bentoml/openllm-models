"phi3:3.8b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: phi3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: microsoft/Phi-3-mini-4k-instruct
    max_model_len: 4096
    dtype: half
  chat_template: phi-3
  extra_labels:
    openllm_alias: 3.8b,3.8b-mini,3.8b-mini-instruct-4k-fp16
    model_name: microsoft/Phi-3-mini-4k-instruct
"llama2:7b-chat-fp16":
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-t4
  engine_config:
    model: meta-llama/Llama-2-7b-chat-hf
    max_model_len: 1024
    dtype: half
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 7b,7b-chat
    model_name: meta-llama/Llama-2-7b-chat-hf
"llama2:13b-chat-fp16":
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
  engine_config:
    model: meta-llama/Llama-2-13b-chat-hf
    max_model_len: 1024
    dtype: half
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 13b,13b-chat
    model_name: meta-llama/Llama-2-13b-chat-hf
"llama2:70b-chat-fp16":
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: meta-llama/Llama-2-70b-chat-hf
    max_model_len: 1024
    dtype: half
    tensor_parallel_size: 2
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 70b,70b-chat
    model_name: meta-llama/Llama-2-70b-chat-hf
"llama2:7b-chat-awq-4bit":
  project: vllm-chat
  service_config:
    name: llama2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: TheBloke/Llama-2-7B-Chat-AWQ
    max_model_len: 1024
    quantization: awq
    enforce_eager: true
  chat_template: llama-2-chat
  extra_labels:
    openllm_alias: 7b-4bit,7b-chat-4bit
    model_name: TheBloke/Llama-2-7B-Chat-AWQ
"mistral:7b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: TheBloke/Mistral-7B-Instruct-v0.1-AWQ
    max_model_len: 1024
    quantization: awq
    enforce_eager: true
    dtype: half
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 7b-4bit,7b-instruct-4bit
    model_name: TheBloke/Mistral-7B-Instruct-v0.1-AWQ
"mistral:7b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: mistralai/Mistral-7B-Instruct-v0.1
    max_model_len: 1024
    enforce_eager: true
    dtype: half
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 7b,7b-instruct
    model_name: mistralai/Mistral-7B-Instruct-v0.1
"llama3:8b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: casperhansen/llama-3-8b-instruct-awq
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 8b-4bit,8b-instruct-4bit
    model_name: casperhansen/llama-3-8b-instruct-awq
"llama3:70b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: casperhansen/llama-3-70b-instruct-awq
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 70b-4bit,70b-instruct-4bit
    model_name: casperhansen/llama-3-70b-instruct-awq
"llama3:8b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: meta-llama/Meta-Llama-3-8B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 8b,8b-instruct
    model_name: meta-llama/Meta-Llama-3-8B-Instruct
"llama3:70b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: llama3
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: meta-llama/Meta-Llama-3-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 70b,70b-instruct
    model_name: meta-llama/Meta-Llama-3-70B-Instruct
"llama3.1:8b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: meta-llama/Meta-Llama-3.1-8B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 8b,8b-instruct
    model_name: meta-llama/Meta-Llama-3.1-8B-Instruct
"llama3.1:8b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 8b-4bit,8b-instruct-4bit
    model_name: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
"llama3.1:70b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: meta-llama/Meta-Llama-3.1-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 70b,70b-instruct
    model_name: meta-llama/Meta-Llama-3.1-70B-Instruct
"llama3.1:70b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 70b-4bit,70b-instruct-4bit
    model_name: hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
"llama3.1:405b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80g
  engine_config:
    model: hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4
    max_model_len: 2048
    quantization: awq
    tensor_parallel_size: 4
  extra_labels:
    openllm_alias: 405b-4bit,405b-instruct-4bit
    model_name: hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4
"gemma2:9b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: google/gemma-2-9b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 9b,9b-instruct
    model_name: google/gemma-2-9b-it
  extra_envs:
    - name: VLLM_ATTENTION_BACKEND
      value: FLASHINFER
  extra_requirements:
    - --extra-index-url https://flashinfer.ai/whl/cu121/torch2.3
    - flashinfer
"gemma2:27b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: google/gemma-2-27b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 27b,27b-instruct
    model_name: google/gemma-2-27b-it
  extra_envs:
    - name: VLLM_ATTENTION_BACKEND
      value: FLASHINFER
  extra_requirements:
    - --extra-index-url https://flashinfer.ai/whl/cu121/torch2.3
    - flashinfer
"gemma:2b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: gemma
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: google/gemma-2b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 2b,2b-instruct
    model_name: google/gemma-2b-it
"gemma:7b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: gemma
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: google/gemma-7b-it
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 7b,7b-instruct
    model_name: google/gemma-7b-it
"gemma:7b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: gemma
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: casperhansen/gemma-7b-it-awq
    max_model_len: 2048
    quantization: awq
  chat_template: gemma-it
  extra_labels:
    openllm_alias: 7b-4bit,7b-instruct-4bit
    model_name: casperhansen/gemma-7b-it-awq
"mixtral:8x7b-instruct-v0.1-fp16":
  project: vllm-chat
  service_config:
    name: mixtral
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    max_model_len: 2048
    tensor_parallel_size: 2
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 8x7b,8x7b-instruct
    model_name: mistralai/Mixtral-8x7B-Instruct-v0.1
"mixtral:8x7b-instruct-v0.1-awq-4bit":
  project: vllm-chat
  service_config:
    name: mixtral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
  engine_config:
    model: casperhansen/mixtral-instruct-awq
    max_model_len: 2048
    quantization: awq
    gpu_memory_utilization: 0.8
  chat_template: mistral-instruct
  extra_labels:
    openllm_alias: 8x7b-4bit
    model_name: casperhansen/mixtral-instruct-awq
"qwen2:0.5b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2-0.5B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 0.5b,0.5b-instruct
    model_name: Qwen/Qwen2-0.5B-Instruct
"qwen2:1.5b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2-1.5B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 1.5b,1.5b-instruct
    model_name: Qwen/Qwen2-1.5B-Instruct
"qwen2:7b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: Qwen/Qwen2-7B-Instruct-AWQ
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 7b-4bit,7b-instruct-4bit
    model_name: Qwen/Qwen2-7B-Instruct-AWQ
"qwen2:7b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: Qwen/Qwen2-7B-Instruct
    max_model_len: 2048
    dtype: half
  extra_labels:
    openllm_alias: 7b,7b-instruct
    model_name: Qwen/Qwen2-7B-Instruct
"qwen2:72b-instruct-awq-4bit":
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2-72B-Instruct-AWQ
    max_model_len: 2048
    quantization: awq
  extra_labels:
    openllm_alias: 72b-4bit,72b-instruct-4bit
    model_name: Qwen/Qwen2-72B-Instruct-AWQ
"qwen2:57b-a14b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2-57B-A14B-Instruct
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 57b-a14b,57b-a14b-instruct
    model_name: Qwen/Qwen2-57B-A14B-Instruct
"qwen2:72b-instruct-fp16":
  project: vllm-chat
  service_config:
    name: qwen2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80g
  engine_config:
    model: Qwen/Qwen2-72B-Instruct
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 2
  extra_labels:
    openllm_alias: 72b,72b-instruct
    model_name: Qwen/Qwen2-72B-Instruct
"phi3:3.8b-instruct-ggml-q4":
  project: llamacpp-chat
  service_config:
    name: phi3
    traffic:
      timeout: 300
    resources:
      memory: 3Gi
  engine_config:
    model: microsoft/Phi-3-mini-4k-instruct-gguf
    max_model_len: 2048
  chat_template: phi-3
  extra_labels:
    openllm_alias: 3.8b-ggml-q4,3.8b-mini-instruct-4k-ggml-q4
    model_name: microsoft/Phi-3-mini-4k-instruct-gguf
