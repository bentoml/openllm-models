'phi4:14b':
  project: vllm-chat
  service_config:
    name: phi4
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: microsoft/phi-4
    max_model_len: 4096
    dtype: half
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set offset = 1 %}\n{% else %}\n    {% set offset = 0 %}\n{% endif %}\n\n{% for message in messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == offset) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {{ '<|' + message['role'] + '|>\\n' + message['content'].strip() + '<|end|>' + '\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{ '<|assistant|>\\n' }}\n    {% endif %}\n{% endfor %}\n"
'mistral:8b-instruct':
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: mistralai/Ministral-8B-Instruct-2410
    dtype: half
  extra_envs:
    - name: HF_TOKEN
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{{ bos_token }}\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if message['role'] == 'user' %}\n        {{ '[INST] ' + content.strip() + ' [/INST]' }}\n    {% elif message['role'] == 'assistant' %}\n        {{ ' ' + content.strip() + eos_token }}\n    {% endif %}\n{% endfor %}\n"
'mistralai:24b-small-instruct-2501':
  project: vllm-chat
  service_config:
    name: mistral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: mistralai/Mistral-Small-24B-Instruct-2501
  extra_envs:
    - name: HF_TOKEN
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{{ bos_token }}\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if message['role'] == 'user' %}\n        {{ '[INST] ' + content.strip() + ' [/INST]' }}\n    {% elif message['role'] == 'assistant' %}\n        {{ ' ' + content.strip() + eos_token }}\n    {% endif %}\n{% endfor %}\n"
'mistral-large:123b-instruct':
  project: vllm-chat
  service_config:
    name: mistral-large
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: mistralai/Mistral-Large-Instruct-2407
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 4
  extra_envs:
    - name: HF_TOKEN
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{{ bos_token }}\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if message['role'] == 'user' %}\n        {{ '[INST] ' + content.strip() + ' [/INST]' }}\n    {% elif message['role'] == 'assistant' %}\n        {{ ' ' + content.strip() + eos_token }}\n    {% endif %}\n{% endfor %}\n"
'mistral-large:123b-instruct-awq-4bit':
  project: vllm-chat
  service_config:
    name: mistral-large
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: casperhansen/mistral-large-instruct-2407-awq
    max_model_len: 2048
    dtype: half
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{{ bos_token }}\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if message['role'] == 'user' %}\n        {{ '[INST] ' + content.strip() + ' [/INST]' }}\n    {% elif message['role'] == 'assistant' %}\n        {{ ' ' + content.strip() + eos_token }}\n    {% endif %}\n{% endfor %}\n"
'llama3.1:8b-instruct':
  project: vllm-chat
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  extra_envs:
    - name: HF_TOKEN
  engine_config:
    model: meta-llama/Meta-Llama-3.1-8B-Instruct
    max_model_len: 2048
    dtype: half
'gemma2:2b-instruct':
  project: vllm-chat
  service_config:
    name: gemma
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-rtx-3060
  engine_config:
    model: google/gemma-2-2b-it
    max_model_len: 2048
    dtype: half
  extra_envs:
    - name: HF_TOKEN
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
'gemma2:9b-instruct':
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: google/gemma-2-9b-it
    max_model_len: 2048
    dtype: half
  extra_envs:
    - name: VLLM_ATTENTION_BACKEND
      value: FLASHINFER
    - name: HF_TOKEN
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
'gemma2:27b-instruct':
  project: vllm-chat
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: google/gemma-2-27b-it
    max_model_len: 2048
    dtype: half
  extra_envs:
    - name: VLLM_ATTENTION_BACKEND
      value: FLASHINFER
    - name: HF_TOKEN
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
'mixtral:8x7b-instruct-v0.1':
  project: vllm-chat
  service_config:
    name: mixtral
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    max_model_len: 2048
    tensor_parallel_size: 2
  extra_envs:
    - name: HF_TOKEN
'qwen2.5:7b-instruct':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-7B-Instruct
    max_model_len: 2048
    dtype: half
'qwen2.5:14b-instruct':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-14B-Instruct
    max_model_len: 2048
    dtype: half
'qwen2.5:32b-instruct':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-32B-Instruct
    max_model_len: 2048
    dtype: half
'qwen2.5:72b-instruct':
  project: vllm-chat
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct
    max_model_len: 2048
    dtype: half
'llama3.2:1b-instruct':
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: meta-llama/Llama-3.2-1B-Instruct
    max_model_len: 16384
  extra_envs:
    - name: HF_TOKEN
'llama3.2:3b-instruct':
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: meta-llama/Llama-3.2-3B-Instruct
    max_model_len: 16384
  extra_envs:
    - name: HF_TOKEN
'llama3.2:11b-vision-instruct':
  project: vllm-chat
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: meta-llama/Llama-3.2-11B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
  vision: true
  extra_envs:
    - name: HF_TOKEN
'llama3.2:90b-vision-instruct':
  project: vllm-chat
  vision: true
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: meta-llama/Llama-3.2-90B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
  extra_envs:
    - name: HF_TOKEN
'pixtral:12b-240910':
  project: vllm-chat
  vision: true
  service_config:
    name: pixtral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: mistral-community/pixtral-12b-240910
    tokenizer_mode: mistral
    enable_prefix_caching: true
    enable_chunked_prefill: false
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
  server_config:
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{{ bos_token }}\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if message['role'] == 'user' %}\n        {{ '[INST] ' + content.strip() + ' [/INST]' }}\n    {% elif message['role'] == 'assistant' %}\n        {{ ' ' + content.strip() + eos_token }}\n    {% endif %}\n{% endfor %}\n"
  extra_requirements:
    - mistral_common[opencv]
'qwen2.5:72b-instruct-ggml-q4-darwin':
  project: llamacpp-chat
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-72b-instruct-q4_k_m-00001-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00002-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00003-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00004-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00005-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00006-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00007-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00008-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00009-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00010-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00011-of-00012.gguf
      - qwen2.5-72b-instruct-q4_k_m-00012-of-00012.gguf
    filename: qwen2.5-72b-instruct-q4_k_m-00001-of-00012.gguf
    repo_id: Qwen/Qwen2.5-72B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_METAL=on"
'qwen2.5:32b-instruct-ggml-darwin':
  project: llamacpp-chat
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_METAL=on"
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-32b-instruct-00001-of-00017.gguf
      - qwen2.5-32b-instruct-00002-of-00017.gguf
      - qwen2.5-32b-instruct-00003-of-00017.gguf
      - qwen2.5-32b-instruct-00004-of-00017.gguf
      - qwen2.5-32b-instruct-00005-of-00017.gguf
      - qwen2.5-32b-instruct-00006-of-00017.gguf
      - qwen2.5-32b-instruct-00007-of-00017.gguf
      - qwen2.5-32b-instruct-00008-of-00017.gguf
      - qwen2.5-32b-instruct-00009-of-00017.gguf
      - qwen2.5-32b-instruct-00010-of-00017.gguf
      - qwen2.5-32b-instruct-00011-of-00017.gguf
      - qwen2.5-32b-instruct-00012-of-00017.gguf
      - qwen2.5-32b-instruct-00013-of-00017.gguf
      - qwen2.5-32b-instruct-00014-of-00017.gguf
      - qwen2.5-32b-instruct-00015-of-00017.gguf
      - qwen2.5-32b-instruct-00016-of-00017.gguf
      - qwen2.5-32b-instruct-00017-of-00017.gguf
    filename: qwen2.5-32b-instruct-00001-of-00017.gguf
    repo_id: Qwen/Qwen2.5-32B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
'qwen2.5:14b-instruct-ggml-q4-darwin':
  project: llamacpp-chat
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_METAL=on"
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-14b-instruct-q4_0-00001-of-00003.gguf
      - qwen2.5-14b-instruct-q4_0-00002-of-00003.gguf
      - qwen2.5-14b-instruct-q4_0-00003-of-00003.gguf
    filename: qwen2.5-14b-instruct-q4_0-00001-of-00003.gguf
    repo_id: Qwen/Qwen2.5-14B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
'qwen2.5:14b-instruct-ggml-q8-darwin':
  project: llamacpp-chat
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_METAL=on"
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-14b-instruct-q8_0-00001-of-00004.gguf
      - qwen2.5-14b-instruct-q8_0-00002-of-00004.gguf
      - qwen2.5-14b-instruct-q8_0-00003-of-00004.gguf
      - qwen2.5-14b-instruct-q8_0-00004-of-00004.gguf
    filename: qwen2.5-14b-instruct-q8_0-00001-of-00004.gguf
    repo_id: Qwen/Qwen2.5-14B-Instruct-GGUF
  service_config:
    name: qwen2.5
    resources:
      memory: 60Gi
    traffic:
      timeout: 300
'qwen2.5:32b-instruct-awq-4bit':
  project: vllm-chat
  engine_config:
    max_model_len: 20480
    model: Qwen/Qwen2.5-32B-Instruct-AWQ
  project: vllm-chat
  service_config:
    name: qwen2.5
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-a100
    traffic:
      timeout: 300
'jamba1.5:mini':
  project: vllm-chat
  service_config:
    name: jamba1.5
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: ai21labs/AI21-Jamba-1.5-Mini
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 4
  extra_envs:
    - name: HF_TOKEN
'qwen2.5vl:3b-instruct':
  project: vllm-chat
  vision: true
  engine_config:
    max_model_len: 2048
    model: Qwen/Qwen2.5-VL-3B-Instruct
  service_config:
    name: qwen2.5vl
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  extra_requirements:
    - qwen-vl-utils[decord]==0.0.8
'qwen2.5vl:7b-instruct':
  project: vllm-chat
  vision: true
  engine_config:
    max_model_len: 2048
    model: Qwen/Qwen2.5-VL-7B-Instruct
  service_config:
    name: qwen2.5vl
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  extra_requirements:
    - qwen-vl-utils[decord]==0.0.8
'qwen2.5-coder:7b-instruct-ggml-darwin':
  project: llamacpp-chat
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_METAL=on"
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-coder-7b-instruct-00001-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00002-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00003-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00004-of-00004.gguf
    filename: qwen2.5-coder-7b-instruct-00001-of-00004.gguf
    repo_id: Qwen/Qwen2.5-Coder-7B-Instruct-GGUF
  service_config:
    name: qwen2.5-coder
    resources:
      memory: 16Gi
    traffic:
      timeout: 300
'qwen2.5-coder:7b-instruct-ggml-linux':
  project: llamacpp-chat
  extra_envs:
    - name: CMAKE_ARGS
      value: "-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"
  engine_config:
    max_model_len: 2048
    additional_files:
      - qwen2.5-coder-7b-instruct-00001-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00002-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00003-of-00004.gguf
      - qwen2.5-coder-7b-instruct-00004-of-00004.gguf
    filename: qwen2.5-coder-7b-instruct-00001-of-00004.gguf
    repo_id: Qwen/Qwen2.5-Coder-7B-Instruct-GGUF
  service_config:
    name: qwen2.5-coder
    resources:
      memory: 16Gi
    traffic:
      timeout: 300
'qwen2.5-coder:7b-instruct':
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-7B-Instruct
    max_model_len: 8192
'qwen2.5-coder:32b-instruct':
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-32B-Instruct
    max_model_len: 8192
"llama3.3:70b-instruct":
  project: vllm-chat
  service_config:
    name: llama3.3
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: meta-llama/Llama-3.3-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
  extra_envs:
    - name: HF_TOKEN
"deepseek-r1-distill:llama3.3-70b-instruct":
  project: vllm-chat
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    tensor_parallel_size: 2
    max_model_len: 4096
  extra_envs:
    - name: HF_TOKEN
  server_config:
    enable_reasoning: true
    reasoning_parser: deepseek_r1
"deepseek-r1-distill:qwen2.5-32b":
  project: vllm-chat
  service_config:
    name: deepseek-r1-distill
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    traffic:
      timeout: 300
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    max_model_len: 4096
  extra_envs:
    - name: HF_TOKEN
  server_config:
    enable_reasoning: true
    reasoning_parser: deepseek_r1
'deepseek-r1-distill:qwen2.5-14b':
  project: vllm-chat
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
    max_model_len: 4096
  extra_envs:
    - name: HF_TOKEN
  server_config:
    enable_reasoning: true
    reasoning_parser: deepseek_r1
'deepseek-r1-distill:llama3.1-8b':
  project: vllm-chat
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
    max_model_len: 4096
  extra_envs:
    - name: HF_TOKEN
  server_config:
    enable_reasoning: true
    reasoning_parser: deepseek_r1
'deepseek-r1-distill:qwen2.5-7b-math':
  project: vllm-chat
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    max_model_len: 4096
  extra_envs:
    - name: HF_TOKEN
  server_config:
    enable_reasoning: true
    reasoning_parser: deepseek_r1
"deepseek-r1:671b":
  project: vllm-chat
  service_config:
    name: deepseek-v3
    traffic:
      timeout: 300
    resources:
      gpu: 16
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: deepseek-ai/DeepSeek-R1
    tensor_parallel_size: 16
    trust_remote_code: True
    max_model_len: 4096
  server_config:
    enable_reasoning: true
    reasoning_parser: deepseek_r1
  extra_envs:
    - name: HF_TOKEN
"deepseek-v3:671b-instruct":
  project: vllm-chat
  service_config:
    name: deepseek-v3
    traffic:
      timeout: 300
    resources:
      gpu: 16
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: deepseek-ai/DeepSeek-V3
    max_model_len: 2048
    tensor_parallel_size: 16
  extra_envs:
    - name: HF_TOKEN
